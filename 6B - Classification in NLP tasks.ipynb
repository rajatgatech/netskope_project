{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 6B\n",
    "The purpose of this demonstration is to reiterate learning objects in lectures 6.2 and 6.4-6.5. After completing this demonstration, you should feel comfortable:\n",
    "- loading and preparing textual data for analysis with classification-type supervised methods\n",
    "- training a variety of classifiers and evaluating performance with accuracy, precision, and recall\n",
    "- fine-tuning classifiers with randomized parameter searches\n",
    "- evaluating the performance of ensemble models, including a Random Forest and LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data, Preprocessing, and Feature Extraction\n",
    "For this demonstration, we are going to use the same data, preprocessing, and tokenization procedures as in the prior demonstration. The main difference this time, though, is that we are going to focus on *sentiment* instead of *sentiment_score*. Let's look at this column and define a variable for our classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitID</th>\n",
       "      <th>author</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_ideas</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tickers</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145610683</td>\n",
       "      <td>shortvolume</td>\n",
       "      <td>564</td>\n",
       "      <td>55841</td>\n",
       "      <td>{'basic': 'Bearish'}</td>\n",
       "      <td>$BCOR ranks 1675 by short volume at 72 pct The...</td>\n",
       "      <td>BCOR</td>\n",
       "      <td>2018-11-19 07:40:31</td>\n",
       "      <td>-0.9391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142498674</td>\n",
       "      <td>GoodNewsBull</td>\n",
       "      <td>2259</td>\n",
       "      <td>115484</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$NFLX Great Price action. Flush the weak on Op...</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>2018-10-24 13:39:31</td>\n",
       "      <td>-0.2136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135730171</td>\n",
       "      <td>livetraderalerts</td>\n",
       "      <td>971</td>\n",
       "      <td>148564</td>\n",
       "      <td>None</td>\n",
       "      <td>$KS 2.5m ago: SEC Current event(s) report - Ot...</td>\n",
       "      <td>KS</td>\n",
       "      <td>2018-08-30 20:22:49</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125482718</td>\n",
       "      <td>WinAllDay</td>\n",
       "      <td>66</td>\n",
       "      <td>1044</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$ADA.X</td>\n",
       "      <td>ADA.X</td>\n",
       "      <td>2018-06-01 16:39:44</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143001861</td>\n",
       "      <td>MBoardman88</td>\n",
       "      <td>6</td>\n",
       "      <td>153</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>@Dman20200 just making sureüëç. Really trying to...</td>\n",
       "      <td>ATOS</td>\n",
       "      <td>2018-10-27 17:30:41</td>\n",
       "      <td>0.8465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>141224945</td>\n",
       "      <td>tygerview</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$CRMD we are almost at $3. So this leads me to...</td>\n",
       "      <td>CRMD</td>\n",
       "      <td>2018-10-12 21:41:38</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>122216361</td>\n",
       "      <td>Berserk74</td>\n",
       "      <td>37</td>\n",
       "      <td>4812</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$CHK chuckie sound like cookie. I like cookie....</td>\n",
       "      <td>CHK</td>\n",
       "      <td>2018-05-03 20:11:21</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>132457414</td>\n",
       "      <td>Nicknar1213</td>\n",
       "      <td>3</td>\n",
       "      <td>358</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$SESN MM's Breaking our Balls.... No worries e...</td>\n",
       "      <td>SESN</td>\n",
       "      <td>2018-08-02 19:25:09</td>\n",
       "      <td>0.8355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>137223056</td>\n",
       "      <td>JimInvestor</td>\n",
       "      <td>2588</td>\n",
       "      <td>24975</td>\n",
       "      <td>{'basic': 'Bearish'}</td>\n",
       "      <td>Damaged üö´parabola  Crumbling üîª  $AMD   Learn p...</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2018-09-13 19:16:23</td>\n",
       "      <td>-0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>122594183</td>\n",
       "      <td>blockchain0</td>\n",
       "      <td>16</td>\n",
       "      <td>876</td>\n",
       "      <td>None</td>\n",
       "      <td>$TEUM mmmm!!! Smoked bear meat! Don't know if ...</td>\n",
       "      <td>TEUM</td>\n",
       "      <td>2018-05-07 23:05:04</td>\n",
       "      <td>-0.8863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         twitID            author  author_followers  author_ideas   \n",
       "0     145610683       shortvolume               564         55841  \\\n",
       "1     142498674      GoodNewsBull              2259        115484   \n",
       "2     135730171  livetraderalerts               971        148564   \n",
       "3     125482718         WinAllDay                66          1044   \n",
       "4     143001861       MBoardman88                 6           153   \n",
       "...         ...               ...               ...           ...   \n",
       "9995  141224945         tygerview                 3            36   \n",
       "9996  122216361         Berserk74                37          4812   \n",
       "9997  132457414       Nicknar1213                 3           358   \n",
       "9998  137223056       JimInvestor              2588         24975   \n",
       "9999  122594183       blockchain0                16           876   \n",
       "\n",
       "                 sentiment                                               text   \n",
       "0     {'basic': 'Bearish'}  $BCOR ranks 1675 by short volume at 72 pct The...  \\\n",
       "1     {'basic': 'Bullish'}  $NFLX Great Price action. Flush the weak on Op...   \n",
       "2                     None  $KS 2.5m ago: SEC Current event(s) report - Ot...   \n",
       "3     {'basic': 'Bullish'}                                             $ADA.X   \n",
       "4     {'basic': 'Bullish'}  @Dman20200 just making sureüëç. Really trying to...   \n",
       "...                    ...                                                ...   \n",
       "9995  {'basic': 'Bullish'}  $CRMD we are almost at $3. So this leads me to...   \n",
       "9996  {'basic': 'Bullish'}  $CHK chuckie sound like cookie. I like cookie....   \n",
       "9997  {'basic': 'Bullish'}  $SESN MM's Breaking our Balls.... No worries e...   \n",
       "9998  {'basic': 'Bearish'}  Damaged üö´parabola  Crumbling üîª  $AMD   Learn p...   \n",
       "9999                  None  $TEUM mmmm!!! Smoked bear meat! Don't know if ...   \n",
       "\n",
       "     tickers            timestamp  sentiment_score  \n",
       "0       BCOR  2018-11-19 07:40:31          -0.9391  \n",
       "1       NFLX  2018-10-24 13:39:31          -0.2136  \n",
       "2         KS  2018-08-30 20:22:49           0.0000  \n",
       "3      ADA.X  2018-06-01 16:39:44           0.0000  \n",
       "4       ATOS  2018-10-27 17:30:41           0.8465  \n",
       "...      ...                  ...              ...  \n",
       "9995    CRMD  2018-10-12 21:41:38           0.0000  \n",
       "9996     CHK  2018-05-03 20:11:21           0.0000  \n",
       "9997    SESN  2018-08-02 19:25:09           0.8355  \n",
       "9998     AMD  2018-09-13 19:16:23          -0.6667  \n",
       "9999    TEUM  2018-05-07 23:05:04          -0.8863  \n",
       "\n",
       "[10000 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and fix formatting\n",
    "import pandas as pd, numpy as np, ftfy\n",
    "data_path = \"/storage/ice-shared/mgt8833/classdata/stocktwits_sample.csv.gz\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.replace({np.nan: 'None'}) #ICE loaded these in as \"NaN\"\n",
    "df['text'] = df['text'].apply(lambda x: ftfy.ftfy(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *sentiment* is missing for some observations. Let's look at the count by unique value of *sentiment*. **PAUSE** and generate those counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"{'basic': 'Bearish'}\", \"{'basic': 'Bullish'}\", 'None'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your own work goes here:\n",
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use some `loc` commands to create an indicator variable that takes the value 1 for Bullish, 0 for Bearish, and missing for \"None\". Instead, though, let's use a pandas function that's designed to generate \"one hot\" encodings, `pd.get_dummies()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>None</th>\n",
       "      <th>{'basic': 'Bearish'}</th>\n",
       "      <th>{'basic': 'Bullish'}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       None  {'basic': 'Bearish'}  {'basic': 'Bullish'}\n",
       "0     False                  True                 False\n",
       "1     False                 False                  True\n",
       "2      True                 False                 False\n",
       "3     False                 False                  True\n",
       "4     False                 False                  True\n",
       "...     ...                   ...                   ...\n",
       "9995  False                 False                  True\n",
       "9996  False                 False                  True\n",
       "9997  False                 False                  True\n",
       "9998  False                  True                 False\n",
       "9999   True                 False                 False\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this to our dataframe with `pd.concat`, remove observations with the value \"None\" (though keep in another dataframe), and rename the Bullish column to be simply \"Y\", since that's what we're going to use in our classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitID</th>\n",
       "      <th>author</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_ideas</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tickers</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>None</th>\n",
       "      <th>{'basic': 'Bearish'}</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145610683</td>\n",
       "      <td>shortvolume</td>\n",
       "      <td>564</td>\n",
       "      <td>55841</td>\n",
       "      <td>{'basic': 'Bearish'}</td>\n",
       "      <td>$BCOR ranks 1675 by short volume at 72 pct The...</td>\n",
       "      <td>BCOR</td>\n",
       "      <td>2018-11-19 07:40:31</td>\n",
       "      <td>-0.9391</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142498674</td>\n",
       "      <td>GoodNewsBull</td>\n",
       "      <td>2259</td>\n",
       "      <td>115484</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$NFLX Great Price action. Flush the weak on Op...</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>2018-10-24 13:39:31</td>\n",
       "      <td>-0.2136</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125482718</td>\n",
       "      <td>WinAllDay</td>\n",
       "      <td>66</td>\n",
       "      <td>1044</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$ADA.X</td>\n",
       "      <td>ADA.X</td>\n",
       "      <td>2018-06-01 16:39:44</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143001861</td>\n",
       "      <td>MBoardman88</td>\n",
       "      <td>6</td>\n",
       "      <td>153</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>@Dman20200 just making sureüëç. Really trying to...</td>\n",
       "      <td>ATOS</td>\n",
       "      <td>2018-10-27 17:30:41</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120100505</td>\n",
       "      <td>dhasone225</td>\n",
       "      <td>6</td>\n",
       "      <td>368</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$NFLX My guess is there will be a huge green v...</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>2018-04-16 17:55:35</td>\n",
       "      <td>0.9582</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>134884001</td>\n",
       "      <td>slowslimslider</td>\n",
       "      <td>234</td>\n",
       "      <td>18289</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$SPY Bulls in firm control; no need to fight i...</td>\n",
       "      <td>SPY</td>\n",
       "      <td>2018-08-23 14:38:19</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>141224945</td>\n",
       "      <td>tygerview</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$CRMD we are almost at $3. So this leads me to...</td>\n",
       "      <td>CRMD</td>\n",
       "      <td>2018-10-12 21:41:38</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>122216361</td>\n",
       "      <td>Berserk74</td>\n",
       "      <td>37</td>\n",
       "      <td>4812</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$CHK chuckie sound like cookie. I like cookie....</td>\n",
       "      <td>CHK</td>\n",
       "      <td>2018-05-03 20:11:21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>132457414</td>\n",
       "      <td>Nicknar1213</td>\n",
       "      <td>3</td>\n",
       "      <td>358</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$SESN MM's Breaking our Balls.... No worries e...</td>\n",
       "      <td>SESN</td>\n",
       "      <td>2018-08-02 19:25:09</td>\n",
       "      <td>0.8355</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>137223056</td>\n",
       "      <td>JimInvestor</td>\n",
       "      <td>2588</td>\n",
       "      <td>24975</td>\n",
       "      <td>{'basic': 'Bearish'}</td>\n",
       "      <td>Damaged üö´parabola  Crumbling üîª  $AMD   Learn p...</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2018-09-13 19:16:23</td>\n",
       "      <td>-0.6667</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3937 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         twitID          author  author_followers  author_ideas   \n",
       "0     145610683     shortvolume               564         55841  \\\n",
       "1     142498674    GoodNewsBull              2259        115484   \n",
       "3     125482718       WinAllDay                66          1044   \n",
       "4     143001861     MBoardman88                 6           153   \n",
       "5     120100505      dhasone225                 6           368   \n",
       "...         ...             ...               ...           ...   \n",
       "9993  134884001  slowslimslider               234         18289   \n",
       "9995  141224945       tygerview                 3            36   \n",
       "9996  122216361       Berserk74                37          4812   \n",
       "9997  132457414     Nicknar1213                 3           358   \n",
       "9998  137223056     JimInvestor              2588         24975   \n",
       "\n",
       "                 sentiment                                               text   \n",
       "0     {'basic': 'Bearish'}  $BCOR ranks 1675 by short volume at 72 pct The...  \\\n",
       "1     {'basic': 'Bullish'}  $NFLX Great Price action. Flush the weak on Op...   \n",
       "3     {'basic': 'Bullish'}                                             $ADA.X   \n",
       "4     {'basic': 'Bullish'}  @Dman20200 just making sureüëç. Really trying to...   \n",
       "5     {'basic': 'Bullish'}  $NFLX My guess is there will be a huge green v...   \n",
       "...                    ...                                                ...   \n",
       "9993  {'basic': 'Bullish'}  $SPY Bulls in firm control; no need to fight i...   \n",
       "9995  {'basic': 'Bullish'}  $CRMD we are almost at $3. So this leads me to...   \n",
       "9996  {'basic': 'Bullish'}  $CHK chuckie sound like cookie. I like cookie....   \n",
       "9997  {'basic': 'Bullish'}  $SESN MM's Breaking our Balls.... No worries e...   \n",
       "9998  {'basic': 'Bearish'}  Damaged üö´parabola  Crumbling üîª  $AMD   Learn p...   \n",
       "\n",
       "     tickers            timestamp  sentiment_score   None   \n",
       "0       BCOR  2018-11-19 07:40:31          -0.9391  False  \\\n",
       "1       NFLX  2018-10-24 13:39:31          -0.2136  False   \n",
       "3      ADA.X  2018-06-01 16:39:44           0.0000  False   \n",
       "4       ATOS  2018-10-27 17:30:41           0.8465  False   \n",
       "5       NFLX  2018-04-16 17:55:35           0.9582  False   \n",
       "...      ...                  ...              ...    ...   \n",
       "9993     SPY  2018-08-23 14:38:19           0.6667  False   \n",
       "9995    CRMD  2018-10-12 21:41:38           0.0000  False   \n",
       "9996     CHK  2018-05-03 20:11:21           0.0000  False   \n",
       "9997    SESN  2018-08-02 19:25:09           0.8355  False   \n",
       "9998     AMD  2018-09-13 19:16:23          -0.6667  False   \n",
       "\n",
       "      {'basic': 'Bearish'}      Y  \n",
       "0                     True  False  \n",
       "1                    False   True  \n",
       "3                    False   True  \n",
       "4                    False   True  \n",
       "5                    False   True  \n",
       "...                    ...    ...  \n",
       "9993                 False   True  \n",
       "9995                 False   True  \n",
       "9996                 False   True  \n",
       "9997                 False   True  \n",
       "9998                  True  False  \n",
       "\n",
       "[3937 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([df,pd.get_dummies(df['sentiment'])],axis=1)\n",
    "no_sent = df2.loc[df2['None']==1]\n",
    "df2 = df2.loc[df2['None']==0].rename(columns = {\"{'basic': 'Bullish'}\":\"Y\"})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to the rest of our set up procedures. We will keep everything the same as in Demo 6A:\n",
    "1. Use custom tokenizer that is meant to work with tweets and addresses cash tags. \n",
    "2. Remove stop words and perform other normalization / preprocessing steps.\n",
    "3. Use TF-IDF weighting to extract features. We'll allow for single words and bigrams, but only retain the top 1,000 features since we have less data. I'm also going to limit the matrix to words that appear in no more than 50% of tweets.\n",
    "4. Split the data into a training and validation set.\n",
    "The following cells run through this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll set up our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up tokenizer\n",
    "import emoji, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "cashtag_rx = re.compile(r'\\$[a-z0-9.]+?\\b',re.I)\n",
    "\n",
    "twtokenize = TweetTokenizer()\n",
    "\n",
    "def myTweetTokenizer(tweet):\n",
    "    tweet = re.sub(cashtag_rx,'cash_tag',tweet) # get rid of cash tags\n",
    "    toks = twtokenize.tokenize(tweet)\n",
    "    good_tokens = []\n",
    "    for tok in toks:\n",
    "        if emoji.is_emoji(tok):\n",
    "            good_tokens.append(tok)\n",
    "        elif tok=='cash_tag':\n",
    "            good_tokens.append(tok)        \n",
    "        elif len(tok)>=3 and tok.isalpha():\n",
    "            good_tokens.append(tok.lower())\n",
    "        elif tok.startswith(\"#\"):\n",
    "            good_tokens.append(tok.lower()) #we'll keep lowercase hashtags    \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return good_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll extract our features (and check the top 10 words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words based on word counts:\n",
      "\n",
      "buy, today, get, cash_tag cash_tag, going, like, good, short, day, time\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(lowercase = False, tokenizer = myTweetTokenizer, ngram_range = (1,2), max_features = 1000, stop_words = stops, max_df=0.50)\n",
    "\n",
    "dtm = tfidf_vec.fit_transform(df2['text'])\n",
    "\n",
    "vocab = np.asarray(tfidf_vec.get_feature_names_out())\n",
    "\n",
    "topn = 10\n",
    "print(f\"{topn} most common words based on word counts:\\n\")\n",
    "words = vocab[np.asarray(dtm.todense()).sum(axis=0).argsort()[-topn:][::-1]]\n",
    "print(\", \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX,validX,trainy,validy = train_test_split(dtm, df2['Y'],train_size=0.80,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Simple Classifiers with `sklearn`\n",
    "We'll begin by training and evaluating performance of two common classifiers for NLP-related tasks, Decision Trees and the Naive Bayes Classifier (NBC). You'll notice the process for training these models is very similar to the approach we used in our regression tasks, though obviously the hyperparameters we set in each classifier vary.\n",
    "#### Decision Trees ####\n",
    "For the __[Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)__, we'll consider the following hyper parameters:\n",
    "- `criterion`: how the quality of the \"split\" is measured (or how \"good\" a given leave is); we will leave this as the default (\"gini\") for now.\n",
    "- `max_depth`: how big the tree can get; defaults to `None`, which guarantees an overfit tree. We'll use 200.\n",
    "- `max_features`: how many features the tree can use. We'll use \"sqrt\", which uses the square root of the number of features.\n",
    "- `class_weight`: whether to assign unequal weights to observations. We won't use this yet, but we'll come back to it later.\n",
    "\n",
    "To evaluate the quality of the fit, there are a number of metrics we can use. We're going to use two functions, `confusion_matrix` and `classification_report`. The former is a contingency table, or 2x2 presentation of Type I and Type II errors. The latter provides detailed information on precision, recall, and the overall F1 score.\n",
    "\n",
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=200, max_features=&#x27;sqrt&#x27;, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=200, max_features=&#x27;sqrt&#x27;, random_state=123)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=200, max_features='sqrt', random_state=123)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "tree = DTC(max_depth = 200, max_features = 'sqrt', random_state = 123)\n",
    "tree.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn` metrics commands accept two position arguments, `y_true` and `y_pred`. The \"true\" values, `y_true`, are simply the assigned labels for the data we are classifying. The predicted values, `y_pred`, should be generated from our fitted model.\n",
    "\n",
    "We will now generate the __[`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)__ and __[`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 475,   95],\n",
       "       [  16, 2563]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(trainy,tree.predict(trainX))\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix presents the true values as rows, and predicted values as columns. To illustrate, compute the number of true positives (the bottom row) and compare to the number of true positives in `trainy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1,:].sum()==trainy.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, accurate models have lots of observations on the diagonal, and few off diagonal. We can easily compute overall accuracy by comparing the number of observations on the diagonal with the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9647507145125437"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cm[0,0] + cm[1,1])/cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOW!!**\n",
    "\n",
    "Is there a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7423857868020305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(validy,tree.predict(validX))\n",
    "\n",
    "(cm[0,0] + cm[1,1])/cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model still performs relatively well, though not nearly as good as on the trained data. But that makes sense--decision trees **will** eventually **perfectly** fit the data if given enough leeway. We limited the number of features and depth, so perfect prediction didn't happen, but it was close.\n",
    "\n",
    "Recall that precision measures the quality of the classification from a \"signal to noise\" perspective. Recall measures the quality of the model based on it's ability to avoid \"false negatives\".  Often recall may be more important than precision in a given model, or vice versa. For instance, fire fighters check on pretty much all automated alarm calls (if not cancelled by customer) even though the majority are false alarms. \n",
    "\n",
    "Let's compute precision and recall for the 1s in this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 81.74%.\n",
      "Recall is 86.61%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision is {round((cm[1,1]/cm[:,1].sum())*100,2)}%.\") # bottom right divided by total in right column\n",
    "print(f\"Recall is {round((cm[1,1]/cm[1,:].sum())*100,2)}%.\") # bottom right divided by total in bottom row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are precision and recall both much higher than overall accuracy? \n",
    "\n",
    "The model is **biased**, which we can see if we examine precision and recall for the \"0\" class (how often was a *bearish* prediction correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 36.64%.\n",
      "Recall is 28.57%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision is {round((cm[0,0]/cm[:,0].sum())*100,2)}%.\") # bottom right divided by total in right column\n",
    "print(f\"Recall is {round((cm[0,0]/cm[0,:].sum())*100,2)}%.\") # bottom right divided by total in bottom row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a great way to examine model performance at a very granular level, but we can easily generate metrics we just went through using `sklearn`'s `classification_report`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.37      0.29      0.32       168\n",
      "        True       0.82      0.87      0.84       620\n",
      "\n",
      "    accuracy                           0.74       788\n",
      "   macro avg       0.59      0.58      0.58       788\n",
      "weighted avg       0.72      0.74      0.73       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validy,tree.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've talked about precision, recall, and accuracy (a model level metric). Let's run through the other items on this report:\n",
    "- `f1-score`: the geometric mean of precision and recall (i.e., sqrt(precision\\*recall))\n",
    "- `support`: the number of observations contributing to the given row (i.e., the number of 0s or 1s, or total)\n",
    "- `macro avg`: the average value of the given class-level metric\n",
    "- `weighted avg`: the weighted average value of the given class-level metric (i.e., majority class more heavily influences statistic)\n",
    "\n",
    "There are a number of ways we can try to deal with model bias, such as resampling (under or oversampling), reducing complexity, or, for some models, using class weights. Class weights default to equal, but if we \"value\" minority class observations more, then then model will shift parameters towards more accurate classification of those at the expense of the majority class. \n",
    "\n",
    "Class weights are a hyperparamter that can be tuned. Let's see what the model looks like if we try to approximate roughly equal weights. which we can do by setting the `class_weight` hyperparameter equal to \"balanced\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;, max_depth=200,\n",
       "                       max_features=&#x27;sqrt&#x27;, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;, max_depth=200,\n",
       "                       max_features=&#x27;sqrt&#x27;, random_state=123)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', max_depth=200,\n",
       "                       max_features='sqrt', random_state=123)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DTC(max_depth = 200, max_features = 'sqrt', random_state = 123,class_weight='balanced')\n",
    "tree.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.36      0.29      0.32       168\n",
      "        True       0.82      0.86      0.84       620\n",
      "\n",
      "    accuracy                           0.74       788\n",
      "   macro avg       0.59      0.57      0.58       788\n",
      "weighted avg       0.72      0.74      0.73       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validy,tree.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't help at all. We can try more aggressive class weights using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.27      0.60      0.38       168\n",
      "        True       0.84      0.57      0.68       620\n",
      "\n",
      "    accuracy                           0.58       788\n",
      "   macro avg       0.56      0.58      0.53       788\n",
      "weighted avg       0.72      0.58      0.62       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cws = {0:10,1:1}\n",
    "tree = DTC(max_depth = 200, max_features = 'sqrt', random_state = 123,class_weight=cws)\n",
    "tree.fit(trainX,trainy)\n",
    "print(classification_report(validy,tree.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did slightly change performance, but it doesn't seem to be materially better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier\n",
    "Scikit learn offers several implementations of the Naive Bayes (NB) models. For classification, we'll focus on the __[Gaussian NB model](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)__. This model is much simpler to implement since it has few hyperparameters. In fact, there are only 2:\n",
    "- `priors`: only updated if we have prior probability expectations that differ from the data (we don't)\n",
    "- `var_smoothing`: a smoothing parameter we'll leave as the default\n",
    "\n",
    "So let's implement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.24      0.63      0.35       168\n",
      "        True       0.82      0.46      0.59       620\n",
      "\n",
      "    accuracy                           0.50       788\n",
      "   macro avg       0.53      0.55      0.47       788\n",
      "weighted avg       0.70      0.50      0.54       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as NBC\n",
    "nbc = NBC()\n",
    "nbc.fit(trainX.toarray(),trainy)\n",
    "print(classification_report(validy,nbc.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model did not work well. Precision for the \"1s\" is pretty good, but otherwise it's not great.\n",
    "\n",
    "We can try to adjust the `var_smoothing` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.36      0.52      0.43       168\n",
      "        True       0.85      0.75      0.80       620\n",
      "\n",
      "    accuracy                           0.70       788\n",
      "   macro avg       0.61      0.64      0.61       788\n",
      "weighted avg       0.75      0.70      0.72       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc = NBC(var_smoothing=1)\n",
    "nbc.fit(trainX.toarray(),trainy)\n",
    "print(classification_report(validy,nbc.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs much better now, but like before it's biased. Before moving on, this is an important illustration of the fact that the default hyperparameters are often far from the best choices, which is why model tuning is so important. We'll return to a tuning exercise towards the end of this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "As discussed in the lectures, SGD is not, itself, a classification technique. Rather, it's an optimization technique that can improve performance of more traditional classification methods. Ones that historically aren't well suited for text can be adapted for an NLP-related task by increasing the efficiency of optimization.\n",
    "\n",
    "Let's explore how SGD performs using a few different implementations of `sklearn`'s __[SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)__. This classifier implements a variety of \"regularized linear models\" for classification tasks. Some key parameters include the following:\n",
    "- `loss`: This defines the loss function to be used, which really controls what type of model you're implementing. For instance `hinge` (the default) uses a support vector machine, `log_loss` implements a logistic regression, and `modified_huber` a third implementation designed for classification tasks. There are several others as well, which we'll look at below.\n",
    "- `penalty`: This allows you to select the type of regularization penalty to include in the loss function. It defaults to \"L2\" (which is based on sum of squared feature importances). We'll use \"L1\" to encourage sparsity of feature selection.\n",
    "\n",
    "There are many other hyperparameters related to how the model is trained, maximum iterations, tolerances, etc. that we will leave as defaults for now. Let's set up a loop that tries the three primary classification-based loss functions and outputs the model fit statistics. We'll save each model as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SGD model with hinge loss function.\n",
      "Classification Report for this model:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.47      0.26      0.33       168\n",
      "        True       0.82      0.92      0.87       620\n",
      "\n",
      "    accuracy                           0.78       788\n",
      "   macro avg       0.64      0.59      0.60       788\n",
      "weighted avg       0.75      0.78      0.75       788\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running SGD model with log_loss loss function.\n",
      "Classification Report for this model:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.47      0.24      0.31       168\n",
      "        True       0.82      0.93      0.87       620\n",
      "\n",
      "    accuracy                           0.78       788\n",
      "   macro avg       0.64      0.58      0.59       788\n",
      "weighted avg       0.74      0.78      0.75       788\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running SGD model with modified_huber loss function.\n",
      "Classification Report for this model:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.37      0.33      0.35       168\n",
      "        True       0.82      0.85      0.83       620\n",
      "\n",
      "    accuracy                           0.74       788\n",
      "   macro avg       0.59      0.59      0.59       788\n",
      "weighted avg       0.73      0.74      0.73       788\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running SGD model with squared_hinge loss function.\n",
      "Classification Report for this model:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.32      0.35      0.33       168\n",
      "        True       0.82      0.80      0.81       620\n",
      "\n",
      "    accuracy                           0.70       788\n",
      "   macro avg       0.57      0.57      0.57       788\n",
      "weighted avg       0.71      0.70      0.71       788\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running SGD model with perceptron loss function.\n",
      "Classification Report for this model:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.54      0.12      0.20       168\n",
      "        True       0.80      0.97      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.67      0.55      0.54       788\n",
      "weighted avg       0.75      0.79      0.73       788\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "fitted_models = {}\n",
    "loss_functions = ['hinge','log_loss','modified_huber', 'squared_hinge', 'perceptron']\n",
    "for loss in loss_functions:\n",
    "    sgd = SGD(loss=loss, penalty='l1', n_jobs=-1,random_state=123)\n",
    "    print(f\"Running SGD model with {loss} loss function.\")\n",
    "    sgd.fit(trainX.toarray(),trainy)\n",
    "    print(f\"Classification Report for this model:\\n\\n\")\n",
    "    print(classification_report(validy,sgd.predict(validX.toarray())))\n",
    "    fitted_models[loss] = sgd\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that performance is good to very good for classifying bulls, but the bears are much harder to classify. On the one hand, this is to be expected since there's much less data available for those observations (hence the name \"minority class\"). On the other hand, some of this could be due to overfitting and balanced class weights. We'll do some more robust tuning later.\n",
    "\n",
    "Since these are inherently linear models, so we can explore the features that are most important in each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the hinge SGD model, the following 10 features were most important:\n",
      "\n",
      "\tputs has a coefficient of -6.902199813969005\n",
      "\tüöÄ has a coefficient of 6.675852956121767\n",
      "\tcash_tag even has a coefficient of 5.289324050186542\n",
      "\tprice target has a coefficient of -5.007159675075177\n",
      "\tloading has a coefficient of 4.996937143541227\n",
      "\tblack has a coefficient of -4.956224936444926\n",
      "\tsure has a coefficient of 4.955262327903679\n",
      "\tbreakdown has a coefficient of -4.866290421255361\n",
      "\tfalling has a coefficient of -4.782478304917979\n",
      "\tlose has a coefficient of -4.744034073990065\n",
      "\n",
      "\n",
      "For the log_loss SGD model, the following 10 features were most important:\n",
      "\n",
      "\tputs has a coefficient of -9.681424360673088\n",
      "\tlose has a coefficient of -5.726389815340484\n",
      "\taction has a coefficient of 5.471484784929074\n",
      "\tbreakdown has a coefficient of -5.422774470177812\n",
      "\tüöÄ has a coefficient of 5.418731521757557\n",
      "\tdrop has a coefficient of -5.026357519808267\n",
      "\tcash_tag even has a coefficient of 4.944002597325357\n",
      "\tgarbage has a coefficient of -4.901348382846325\n",
      "\tfalling has a coefficient of -4.750033222911361\n",
      "\tdump has a coefficient of -4.681362928575245\n",
      "\n",
      "\n",
      "For the modified_huber SGD model, the following 10 features were most important:\n",
      "\n",
      "\tcash_tag puts has a coefficient of -29.501103229125352\n",
      "\tcash_tag bear has a coefficient of 28.058753375363356\n",
      "\tgreat has a coefficient of 27.487104351450512\n",
      "\tputs has a coefficient of -25.658825552084455\n",
      "\tcash_tag day has a coefficient of 25.3240339321474\n",
      "\tclimbing has a coefficient of 23.676156577937196\n",
      "\tbreakdown has a coefficient of -23.643410406208513\n",
      "\tcash_tag adding has a coefficient of 22.770082149838956\n",
      "\tdeal has a coefficient of 22.68743181712014\n",
      "\teow has a coefficient of 22.25826171686702\n",
      "\n",
      "\n",
      "For the squared_hinge SGD model, the following 10 features were most important:\n",
      "\n",
      "\taction has a coefficient of 9514754475660.793\n",
      "\tbreakdown has a coefficient of -8561131182440.498\n",
      "\tüöÄ has a coefficient of 7941889327307.098\n",
      "\tinteresting has a coefficient of 7730704383888.438\n",
      "\tcash_tag even has a coefficient of 7337936556928.843\n",
      "\tdeal has a coefficient of 7247886877180.207\n",
      "\tinterest has a coefficient of 7171371279296.395\n",
      "\tcash_tag adding has a coefficient of 7131501589543.477\n",
      "\tfly has a coefficient of 7064955892959.274\n",
      "\teow has a coefficient of 7003546481300.492\n",
      "\n",
      "\n",
      "For the perceptron SGD model, the following 10 features were most important:\n",
      "\n",
      "\tloading has a coefficient of 5.420733012072894\n",
      "\tcash_tag day has a coefficient of 3.210682627040341\n",
      "\tposted has a coefficient of 3.0377446798898573\n",
      "\theld has a coefficient of 2.9929836096245124\n",
      "\tüêÇ has a coefficient of 2.9073380473705424\n",
      "\tüí∞ has a coefficient of 2.8471811071707784\n",
      "\taction has a coefficient of 2.7031031142751285\n",
      "\tbreakdown has a coefficient of -2.6153748641232304\n",
      "\tüöÄ has a coefficient of 2.598931237984223\n",
      "\tcash_tag adding has a coefficient of 2.5721946814632286\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topn = 10\n",
    "for loss in loss_functions:\n",
    "    coefs = fitted_models[loss].coef_\n",
    "    print(f\"For the {loss} SGD model, the following {topn} features were most important:\\n\")\n",
    "    for c in np.abs(coefs).argsort()[0][::-1][:topn]: # computes absolute values, and argsorts, flips to descending, and grabs first \"n\"\n",
    "        print(f\"\\t{vocab[c]} has a coefficient of {coefs[0][c]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, suppose you wanted to see the 5 most positive, and 5 most negative feature importances. How could you adapt the procedure above to print two separate sets of coefficients? **PAUSE** and try to answer that question on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the hinge SGD model, the following 5 features had most positive predictive values (bullish):\n",
      "\n",
      "\tüöÄ has a coefficient of 6.675852956121767\n",
      "\tcash_tag even has a coefficient of 5.289324050186542\n",
      "\tloading has a coefficient of 4.996937143541227\n",
      "\tsure has a coefficient of 4.955262327903679\n",
      "\tüêÉ has a coefficient of 4.678732972173816\n",
      "For the hinge SGD model, the following 5 features had most negative predictive values (bearish):\n",
      "\n",
      "\tputs has a coefficient of -6.902199813969005\n",
      "\tprice target has a coefficient of -5.007159675075177\n",
      "\tblack has a coefficient of -4.956224936444926\n",
      "\tbreakdown has a coefficient of -4.866290421255361\n",
      "\tfalling has a coefficient of -4.782478304917979\n",
      "\n",
      "\n",
      "For the log_loss SGD model, the following 5 features had most positive predictive values (bullish):\n",
      "\n",
      "\taction has a coefficient of 5.471484784929074\n",
      "\tüöÄ has a coefficient of 5.418731521757557\n",
      "\tcash_tag even has a coefficient of 4.944002597325357\n",
      "\tgreat has a coefficient of 4.325165674008279\n",
      "\tcash_tag day has a coefficient of 4.201290991633398\n",
      "For the log_loss SGD model, the following 5 features had most negative predictive values (bearish):\n",
      "\n",
      "\tputs has a coefficient of -9.681424360673088\n",
      "\tlose has a coefficient of -5.726389815340484\n",
      "\tbreakdown has a coefficient of -5.422774470177812\n",
      "\tdrop has a coefficient of -5.026357519808267\n",
      "\tgarbage has a coefficient of -4.901348382846325\n",
      "\n",
      "\n",
      "For the modified_huber SGD model, the following 5 features had most positive predictive values (bullish):\n",
      "\n",
      "\tcash_tag bear has a coefficient of 28.058753375363356\n",
      "\tgreat has a coefficient of 27.487104351450512\n",
      "\tcash_tag day has a coefficient of 25.3240339321474\n",
      "\tclimbing has a coefficient of 23.676156577937196\n",
      "\tcash_tag adding has a coefficient of 22.770082149838956\n",
      "For the modified_huber SGD model, the following 5 features had most negative predictive values (bearish):\n",
      "\n",
      "\tcash_tag puts has a coefficient of -29.501103229125352\n",
      "\tputs has a coefficient of -25.658825552084455\n",
      "\tbreakdown has a coefficient of -23.643410406208513\n",
      "\tmkt has a coefficient of -17.35644988175032\n",
      "\tgarbage has a coefficient of -17.018192106710185\n",
      "\n",
      "\n",
      "For the squared_hinge SGD model, the following 5 features had most positive predictive values (bullish):\n",
      "\n",
      "\taction has a coefficient of 9514754475660.793\n",
      "\tüöÄ has a coefficient of 7941889327307.098\n",
      "\tinteresting has a coefficient of 7730704383888.438\n",
      "\tcash_tag even has a coefficient of 7337936556928.843\n",
      "\tdeal has a coefficient of 7247886877180.207\n",
      "For the squared_hinge SGD model, the following 5 features had most negative predictive values (bearish):\n",
      "\n",
      "\tbreakdown has a coefficient of -8561131182440.498\n",
      "\tdrops has a coefficient of -5824877811435.72\n",
      "\tmkt has a coefficient of -5591657054475.574\n",
      "\tputs has a coefficient of -5531367070262.629\n",
      "\tgarbage has a coefficient of -5452653109852.251\n",
      "\n",
      "\n",
      "For the perceptron SGD model, the following 5 features had most positive predictive values (bullish):\n",
      "\n",
      "\tloading has a coefficient of 5.420733012072894\n",
      "\tcash_tag day has a coefficient of 3.210682627040341\n",
      "\tposted has a coefficient of 3.0377446798898573\n",
      "\theld has a coefficient of 2.9929836096245124\n",
      "\tüêÇ has a coefficient of 2.9073380473705424\n",
      "For the perceptron SGD model, the following 5 features had most negative predictive values (bearish):\n",
      "\n",
      "\tbreakdown has a coefficient of -2.6153748641232304\n",
      "\tlose has a coefficient of -0.9905752203050613\n",
      "\tgreedy has a coefficient of -0.7638848275732746\n",
      "\tputs has a coefficient of -0.7464387012005398\n",
      "\tmargin has a coefficient of -0.6666967150079489\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your own work goes here:\n",
    "topn = 5\n",
    "for loss in loss_functions:\n",
    "    coefs = fitted_models[loss].coef_\n",
    "    print(f\"For the {loss} SGD model, the following {topn} features had most positive predictive values (bullish):\\n\")\n",
    "    for c in coefs.argsort()[0][::-1][:topn]:\n",
    "        print(f\"\\t{vocab[c]} has a coefficient of {coefs[0][c]}\")\n",
    "        \n",
    "    print(f\"For the {loss} SGD model, the following {topn} features had most negative predictive values (bearish):\\n\")\n",
    "    for c in coefs.argsort()[0][:topn]:\n",
    "        print(f\"\\t{vocab[c]} has a coefficient of {coefs[0][c]}\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models\n",
    "Recall that an ensemble model refers to the class of approaches for using estimates from multiple models to form one final prediction. `sklearn` provides classes that you can use to build customized ensembles:\n",
    "- __[`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)__: Facilitates setting up a \"bag\" of weak learners\n",
    "- __[`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)__: Facilitates setting up recursive set of \"boosted\" learners that learn from errors in previous model.\n",
    "- __[`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier)__: Facilitates setting up stacked model that combines output of first level learners in final predictive model.\n",
    "\n",
    "These objects are great for customizing your own ensemble model. We're going to explore two \"prepackaged\" ensemble models, the Random Forest Classifier and LightGBM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "`sklearn`'s `RandomForecastClassifier` (__[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)__) facilitates modeling a random forest, or a collection of \"weak\" learner decision trees. It has several hyperparameters that require tuning. Many (if not most) of them overlap with the decision tree classifier (e.g., `criterion`, `max_depth`, `max_features`, etc.). \n",
    "\n",
    "The main \"new\" parameters that I focus on are:\n",
    "- `n_estimators`: the size of the forest, or number of trees. Defaults to 100.\n",
    "- `max_samples`: The number of observations to use to train each tree (requires `bootstrap` be `True`, which is the default). The default is None (so all samples), but we can set it to something smaller which can help with overfitting.\n",
    "\n",
    "Let's train a few forests, varying only `n_estimators` and `max_samples` so we can explore how performance and bias changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forest model with 50 estimators and a max of 100% observations per tree.\n",
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       168\n",
      "        True       0.79      1.00      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.39      0.50      0.44       788\n",
      "weighted avg       0.62      0.79      0.69       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       570\n",
      "        True       0.82      1.00      0.90      2579\n",
      "\n",
      "    accuracy                           0.82      3149\n",
      "   macro avg       0.41      0.50      0.45      3149\n",
      "weighted avg       0.67      0.82      0.74      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 50 estimators and a max of 75.0% observations per tree.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.51      0.21      0.30       168\n",
      "        True       0.82      0.94      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.66      0.58      0.59       788\n",
      "weighted avg       0.75      0.79      0.75       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.85      0.90       570\n",
      "        True       0.97      0.99      0.98      2579\n",
      "\n",
      "    accuracy                           0.97      3149\n",
      "   macro avg       0.96      0.92      0.94      3149\n",
      "weighted avg       0.97      0.97      0.97      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 50 estimators and a max of 25.0% observations per tree.\n",
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.50      0.11      0.18       168\n",
      "        True       0.80      0.97      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.65      0.54      0.53       788\n",
      "weighted avg       0.74      0.79      0.73       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.30      0.46       570\n",
      "        True       0.87      1.00      0.93      2579\n",
      "\n",
      "    accuracy                           0.87      3149\n",
      "   macro avg       0.91      0.65      0.69      3149\n",
      "weighted avg       0.88      0.87      0.84      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 100 estimators and a max of 100% observations per tree.\n",
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       168\n",
      "        True       0.79      1.00      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.39      0.50      0.44       788\n",
      "weighted avg       0.62      0.79      0.69       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       570\n",
      "        True       0.82      1.00      0.90      2579\n",
      "\n",
      "    accuracy                           0.82      3149\n",
      "   macro avg       0.41      0.50      0.45      3149\n",
      "weighted avg       0.67      0.82      0.74      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 100 estimators and a max of 75.0% observations per tree.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.51      0.20      0.28       168\n",
      "        True       0.81      0.95      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.66      0.57      0.58       788\n",
      "weighted avg       0.75      0.79      0.75       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.87      0.91       570\n",
      "        True       0.97      0.99      0.98      2579\n",
      "\n",
      "    accuracy                           0.97      3149\n",
      "   macro avg       0.96      0.93      0.94      3149\n",
      "weighted avg       0.97      0.97      0.97      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 100 estimators and a max of 25.0% observations per tree.\n",
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.50      0.11      0.18       168\n",
      "        True       0.80      0.97      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.65      0.54      0.53       788\n",
      "weighted avg       0.74      0.79      0.73       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.29      0.45       570\n",
      "        True       0.86      1.00      0.93      2579\n",
      "\n",
      "    accuracy                           0.87      3149\n",
      "   macro avg       0.90      0.64      0.69      3149\n",
      "weighted avg       0.88      0.87      0.84      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 250 estimators and a max of 100% observations per tree.\n",
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       168\n",
      "        True       0.79      1.00      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.39      0.50      0.44       788\n",
      "weighted avg       0.62      0.79      0.69       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       570\n",
      "        True       0.82      1.00      0.90      2579\n",
      "\n",
      "    accuracy                           0.82      3149\n",
      "   macro avg       0.41      0.50      0.45      3149\n",
      "weighted avg       0.67      0.82      0.74      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 250 estimators and a max of 75.0% observations per tree.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.50      0.21      0.30       168\n",
      "        True       0.82      0.94      0.87       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.66      0.58      0.59       788\n",
      "weighted avg       0.75      0.79      0.75       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.87      0.92       570\n",
      "        True       0.97      0.99      0.98      2579\n",
      "\n",
      "    accuracy                           0.97      3149\n",
      "   macro avg       0.97      0.93      0.95      3149\n",
      "weighted avg       0.97      0.97      0.97      3149\n",
      "\n",
      "------------------------------------------------------------\n",
      "Running Random Forest model with 250 estimators and a max of 25.0% observations per tree.\n",
      "Classification Report for this model using validation data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.10      0.17       168\n",
      "        True       0.80      0.98      0.88       620\n",
      "\n",
      "    accuracy                           0.79       788\n",
      "   macro avg       0.70      0.54      0.53       788\n",
      "weighted avg       0.76      0.79      0.73       788\n",
      "\n",
      "Classification Report for this model using training data:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.27      0.42       570\n",
      "        True       0.86      1.00      0.92      2579\n",
      "\n",
      "    accuracy                           0.87      3149\n",
      "   macro avg       0.91      0.63      0.67      3149\n",
      "weighted avg       0.88      0.87      0.83      3149\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "n_estimators = [50,100,250]\n",
    "max_samples = [1,0.75,0.25]\n",
    "\n",
    "for n in n_estimators:\n",
    "    for m in max_samples:\n",
    "        rf = RFC(n_estimators=n, max_samples=m, n_jobs = -1, random_state=123, class_weight='balanced')\n",
    "        print(f\"Running Random Forest model with {n} estimators and a max of {round(m*100,2)}% observations per tree.\")\n",
    "        rf.fit(trainX.toarray(),trainy)\n",
    "        print(f\"Classification Report for this model using validation data:\\n\\n\")\n",
    "        print(classification_report(validy,rf.predict(validX.toarray())))\n",
    "        print(f\"Classification Report for this model using training data:\\n\\n\")\n",
    "        print(classification_report(trainy,rf.predict(trainX.toarray())))\n",
    "        print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of output to sift through, but this illustrates two things:\n",
    "1. There's significant bias in almost all models. In fact, many predict **0** bearish tweets (hence the error messages).\n",
    "2. Many of the models are severely overfitted. \n",
    "\n",
    "#### Light GBM \n",
    "The final ensemble we'll train is a the \"Light Gradient Boosted Machine\" classification model. I chose to include this one because it frequently performs best in NLP tasks. The Python __[implementation of Light GBM](https://lightgbm.readthedocs.io/en/v3.3.5/index.html)__ is not part of `sklearn`, but it has an `sklearn`-like API available through `LGBMClassifier` (__[docs](https://lightgbm.readthedocs.io/en/v3.3.5/pythonapi/lightgbm.LGBMClassifier.html)__). \n",
    "\n",
    "If you examine the documentation, you'll see that this model has *many* hyperparameters. For brevity, we'll just implement a model with default parameters except one. `reg_alpha` is the L1 regularization term, which defaults to 0. To encourage sparsity I'm going to make that weight 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(random_state=123, reg_alpha=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(random_state=123, reg_alpha=0.01)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(random_state=123, reg_alpha=0.01)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier as LGBM\n",
    "\n",
    "lgbm = LGBM(reg_alpha=0.01,random_state=123,n_jobs=-1)\n",
    "lgbm.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.33      0.08      0.13       168\n",
      "        True       0.79      0.95      0.87       620\n",
      "\n",
      "    accuracy                           0.77       788\n",
      "   macro avg       0.56      0.52      0.50       788\n",
      "weighted avg       0.70      0.77      0.71       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validy,lgbm.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becoming familiar... good overall accuracy, but very biased!\n",
    "\n",
    "Now that we've trained basic implementations of a wide range of models, let's conclude with a comprehensive tuning process that tries to identify the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "We've considered X classifiers:\n",
    "1. Decision Tree\n",
    "2. Naive Bayes\n",
    "3. Stochastic Gradient Descent (with several different loss functions)\n",
    "4. Random Forest\n",
    "5. Light GBM\n",
    "\n",
    "We're going to set up a tuning process which re-examines most of these models, but does the following:\n",
    "1. Considers a wide range of hyperparameter values\n",
    "2. Uses five-fold cross-validation\n",
    "3. Optimizes the model on the \"macro\" f1-score\n",
    "4. Saves the \"best\" model\n",
    "\n",
    "Note that we're going to skip the Naive Bayes classifier since it's a little different in that it has a closed form solution and really only has one parameter to tune.\n",
    "\n",
    "#### Set up hyperparameter options for each model ###\n",
    "To start, let's set up our four sets of hyperparameters. Many of these models allow for varying class weights, so we'll start by establishing a set of those to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.6517654072010692, 1: 0.3482345927989308},\n",
       " {0: 0.8569303325248103, 1: 0.14306966747518973},\n",
       " {0: 0.8865742732178985, 1: 0.11342572678210155},\n",
       " {0: 0.7243426154585544, 1: 0.2756573845414456},\n",
       " {0: 0.6402655151072185, 1: 0.35973448489278154},\n",
       " {0: 0.7884467699377695, 1: 0.21155323006223048},\n",
       " {0: 0.5096179008076922, 1: 0.49038209919230774},\n",
       " {0: 0.6575851307075684, 1: 0.34241486929243165},\n",
       " {0: 0.7595340492578195, 1: 0.24046595074218047},\n",
       " {0: 0.8039412409029247, 1: 0.19605875909707526}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "maj_weight = np.random.uniform(0,0.5,100) # 100 random numbers between 0 and 0.5\n",
    "class_weights = [{0: 1-k, 1:k} for k in maj_weight]\n",
    "class_weights.append('balanced') # adds \"balanced\" as an option\n",
    "class_weights.append(None) # adds no weighting as an option\n",
    "class_weights[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add some model-specific paramters. I've selected up to 5 from each of the model types (usually first 4-5), in addition to class_weights (if available) and a random_state. This is not necessarily comprehensive, but it should allow us to observe a wide range of model configurations. \n",
    "\n",
    "We will set up a dictionary for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {'criterion':['gini','entropy','log_loss'],\n",
    "             'max_depth':[None,10, 20, 50, 100],\n",
    "             'min_samples_split':[2,5,10], # required to split\n",
    "             'max_features':[None,'sqrt','log2'],\n",
    "             'class_weight':class_weights,\n",
    "             'random_state':[123]\n",
    "            }\n",
    "\n",
    "sgd_params = {'loss':['hinge','log_loss','modified_huber', 'squared_hinge', 'perceptron'],\n",
    "              'penalty':['l1','l2','elasticnet',None], # elasticinet is another regularization technique that encourages sparsity\n",
    "              'alpha':[0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "              'l1_ratio':np.random.uniform(0,1,10).tolist(),\n",
    "              'random_state':[123],\n",
    "              'class_weight':class_weights\n",
    "             }     \n",
    "\n",
    "rf_params = {'n_estimators':[10,25,50,100,200],\n",
    "             'criterion':['gini','entropy','log_loss'],\n",
    "             'max_depth':[None,10, 20, 50, 100],\n",
    "             'min_samples_split':[2,5,10], # required to split\n",
    "             'max_features':[None,'sqrt','log2'],\n",
    "             'max_samples':[None,0.10,0.50,0.75],\n",
    "             'class_weight':class_weights,\n",
    "             'random_state':[123]\n",
    "            }\n",
    "\n",
    "lgbm_params = {'boosting_type':['gbdt', 'dart', 'goss'],\n",
    "               'num_leaves':[10,20,30,40,50],\n",
    "               'max_depth':[-1, 10, 20, 50, 100],\n",
    "               'learning_rate':[0.01,0.1,0.2,0.5], # note this is a very important parameter, as it controls how much \"feedback\" the model has. You can also define a function to make it adaptive\n",
    "               'n_estimators':[25,50,75,100,200],\n",
    "               'class_weight':class_weights,\n",
    "               'random_state':[123]\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning with RandomizedSearchCV\n",
    "In the previous demonstration, we used a grid search to identify the best set of parameters. If we did a grid search with this set of parameters with 5-fold cross validation we'd have to run... a lot of models! Instead, we will use a *randomized* parameter search with `RandomizedSearchCV` (__[docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)__). \n",
    "\n",
    "The method for setting up a randomized search is very similar to the grid search we've already done. The main adjustment we have to make is that we'll specify how many parameter combinations to try (since we won't try all). We will also set it up to tune each model and optimize on the macro f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report with hold-out sample for best fit of this model:...\n",
      "\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.32      0.34      0.33       168\n",
      "        True       0.82      0.81      0.81       620\n",
      "\n",
      "    accuracy                           0.71       788\n",
      "   macro avg       0.57      0.57      0.57       788\n",
      "weighted avg       0.71      0.71      0.71       788\n",
      "\n",
      "------------------------------------------------------\n",
      "Classification Report with hold-out sample for best fit of this model:...\n",
      "\n",
      "<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.34      0.40      0.37       168\n",
      "        True       0.83      0.79      0.81       620\n",
      "\n",
      "    accuracy                           0.71       788\n",
      "   macro avg       0.59      0.60      0.59       788\n",
      "weighted avg       0.73      0.71      0.72       788\n",
      "\n",
      "------------------------------------------------------\n",
      "Classification Report with hold-out sample for best fit of this model:...\n",
      "\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.37      0.29      0.33       168\n",
      "        True       0.82      0.87      0.84       620\n",
      "\n",
      "    accuracy                           0.74       788\n",
      "   macro avg       0.60      0.58      0.59       788\n",
      "weighted avg       0.72      0.74      0.73       788\n",
      "\n",
      "------------------------------------------------------\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.819589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556090 -> initscore=0.225309\n",
      "[LightGBM] [Info] Start training from score 0.225309\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.217768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543763 -> initscore=0.175500\n",
      "[LightGBM] [Info] Start training from score 0.175500\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.057907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 3.851809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1651\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.776979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.077433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 116\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448533 -> initscore=-0.206600\n",
      "[LightGBM] [Info] Start training from score -0.206600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.119467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1739\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556090 -> initscore=0.225309\n",
      "[LightGBM] [Info] Start training from score 0.225309\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.120548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1739\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543763 -> initscore=0.175500\n",
      "[LightGBM] [Info] Start training from score 0.175500\n",
      "[LightGBM] [Info] Number of positive: 2064, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 6.688780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1617\n",
      "[LightGBM] [Info] Number of data points in the train set: 2520, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543883 -> initscore=0.175985\n",
      "[LightGBM] [Info] Start training from score 0.175985\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.883286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 116\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2064, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.794881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1617\n",
      "[LightGBM] [Info] Number of data points in the train set: 2520, number of used features: 108\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455767 -> initscore=-0.177396\n",
      "[LightGBM] [Info] Start training from score -0.177396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.835501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1651\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556090 -> initscore=0.225309\n",
      "[LightGBM] [Info] Start training from score 0.225309\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.938695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543763 -> initscore=0.175500\n",
      "[LightGBM] [Info] Start training from score 0.175500\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.832230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Info] Number of positive: 2064, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.764612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1617\n",
      "[LightGBM] [Info] Number of data points in the train set: 2520, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455767 -> initscore=-0.177396\n",
      "[LightGBM] [Info] Start training from score -0.177396\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.663524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1739\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 117\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.382907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448533 -> initscore=-0.206600\n",
      "[LightGBM] [Info] Start training from score -0.206600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 5.046163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556090 -> initscore=0.225309\n",
      "[LightGBM] [Info] Start training from score 0.225309\n",
      "[LightGBM] [Info] Number of positive: 2064, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.798893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1617\n",
      "[LightGBM] [Info] Number of data points in the train set: 2520, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.556210 -> initscore=0.225793\n",
      "[LightGBM] [Info] Start training from score 0.225793\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.435925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1651\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543763 -> initscore=0.175500\n",
      "[LightGBM] [Info] Start training from score 0.175500\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 6.373863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1739\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 5.399658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1651\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.455647 -> initscore=-0.177881\n",
      "[LightGBM] [Info] Start training from score -0.177881\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 6.195597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1739\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 117\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448533 -> initscore=-0.206600\n",
      "[LightGBM] [Info] Start training from score -0.206600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 6.210727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1651\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 109\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448533 -> initscore=-0.206600\n",
      "[LightGBM] [Info] Start training from score -0.206600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2064, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 3.817569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1617\n",
      "[LightGBM] [Info] Number of data points in the train set: 2520, number of used features: 108\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448653 -> initscore=-0.206115\n",
      "[LightGBM] [Info] Start training from score -0.206115\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 2063, number of negative: 456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.099984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2519, number of used features: 116\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483396 -> initscore=-0.066441\n",
      "[LightGBM] [Info] Start training from score -0.066441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod,params \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([DTC(),SGD(),RFC(),LGBM()],[dt_params, sgd_params, rf_params, lgbm_params]): \u001b[38;5;66;03m# zip creates a list of tuples we can use to iterate\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     rand_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(mod,params, \u001b[38;5;66;03m# positional arguments, model and parameter grid\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                                      n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                      scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# see options here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                                      cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                      random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m,\n\u001b[1;32m      9\u001b[0m                                      n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mrand_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report with hold-out sample for best fit of this model:...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(mod))\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/storage/ice-shared/mgt8833/class_environment/week6/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "final_mods = []\n",
    "for mod,params in zip([DTC(),SGD(),RFC(),LGBM()],[dt_params, sgd_params, rf_params, lgbm_params]): # zip creates a list of tuples we can use to iterate\n",
    "    rand_search = RandomizedSearchCV(mod,params, # positional arguments, model and parameter grid\n",
    "                                     n_iter=25,\n",
    "                                     scoring='f1_macro', # see options here: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                                     cv=5,\n",
    "                                     random_state=123,\n",
    "                                     n_jobs=-1)\n",
    "    \n",
    "    rand_search.fit(trainX,trainy)\n",
    "    print(f\"Classification Report with hold-out sample for best fit of this model:...\\n\")\n",
    "    print(type(mod))\n",
    "    print(classification_report(validy,rand_search.predict(validX.toarray())))\n",
    "    print(\"------------------------------------------------------\")\n",
    "    final_mods.append(rand_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.32      0.34      0.33       168\n",
      "        True       0.82      0.81      0.81       620\n",
      "\n",
      "    accuracy                           0.71       788\n",
      "   macro avg       0.57      0.57      0.57       788\n",
      "weighted avg       0.71      0.71      0.71       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_search = final_mods[0].best_estimator_\n",
    "print(classification_report(validy,rand_search.predict(validX.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, all of these classifiers really struggle with the bearish tweets. At this point, I would probably conclude this is simply a noisier class, and it's unlikely we can come up with a model that explains them well. \n",
    "\n",
    "However, what if we were in a scenario where getting those right was *much* more important. It may be hard to understand the intuition here, but consider these other scenarios where we have very imbalanced data:\n",
    "1. Understanding which types of narrative disclosures typically appear in intentionally misstated financial statements\n",
    "2. Identifying which types of news paper stories lead to significant litigation\n",
    "3. Understanding which customer complaints result in complete loss of business/revenue.\n",
    "\n",
    "In all of these cases, the base rate of the scenario we are interested in is fairly low (<1% for intentional misstatements).\n",
    "\n",
    "Let's use example 3 above and come up with a custom formula. We will do so by quantifying penalties for each type of error we will encounter:\n",
    "1. Missed 0 (minority class recall): 100 points\n",
    "2. False 0 (minority class precision): 10 points\n",
    "3. True 1 (correctly identify majority class): -1 point\n",
    "\n",
    "We can compute each of these from a confusion matrix and then incorporate the scoring metric into our randomized search. Note that the function we define must \"look\" like traditional scoring metrics, accepting true `y` first and then `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scorer(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    crit1 = cm[0,1] # missed 0s woudl be classified as 1s, so first row 2nd column\n",
    "    crit2 = cm[1,0] # false 0s would be classified as 0s, but actually 1s. so first column 2nd row\n",
    "    crit3 = cm[1,1] # when we get a \"1\" right\n",
    "    return 100*crit1 + 10*crit2 - crit3 # our score\n",
    "\n",
    "# See how our models we just trained did:\n",
    "for mod in final_mods:\n",
    "    print(my_scorer(validy,mod.predict(validX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before incorporating this into our randomized search, we need to make this function a callable scorer, which `sklearn` accomodates with the `make_scorer` function (__[docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)__). There are a few parameters for this function. The only one we need to adjust is whether a higher score is better (`True` is the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "myscorer = make_scorer(my_scorer,greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can re-run our search with our new scorer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mods2 = []\n",
    "for mod,params in zip([DTC(),SGD(),RFC(),LGBM()],[dt_params, sgd_params, rf_params, lgbm_params]): # zip creates a list of tuples we can use to iterate\n",
    "    rand_search = RandomizedSearchCV(mod,params, # positional arguments, model and parameter grid\n",
    "                                     n_iter=25,\n",
    "                                     scoring=myscorer,\n",
    "                                     cv=5,\n",
    "                                     random_state=123,\n",
    "                                     n_jobs=-1)\n",
    "    \n",
    "    rand_search.fit(trainX,trainy)\n",
    "    print(f\"Classification Report with hold-out sample for best fit of this model:...\\n\")\n",
    "    print(type(mod))\n",
    "    print(classification_report(validy,rand_search.predict(validX.toarray())))\n",
    "    print(\"------------------------------------------------------\")\n",
    "    final_mods2.append(rand_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall model performance declined (by design), but these do appear to do much better with the minority class, at least with respect to recall (which is what had the highest penalty). Let's see what parameters produced the best fit for the LGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mods2[3].best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we'll cover on NLP-based classifiers in this module. The concepts will come up again when we discuss deep learning, though.\n",
    "\n",
    "Before we conclude, I want to highlight that we did not thoroughly examine all means of optimizing the performance of these models. For instance, we could have:\n",
    "1. Changed the number of features (more or less)\n",
    "2. Applied dimensionality reduction techniques\n",
    "3. Standardized or normalized our data (would impact the non-tree based models)\n",
    "4. Tried resampling (we did not cover this, but oversampling might have improved model performance here by adding synthetic observations to the minority class)\n",
    "\n",
    "**HOMEWORK**\n",
    "I'd like each of you to think about a scenario in your own professional experience where a classifier such as the one we trained may be useful--do you expect class imbalance in your scenario? If so, which class likely bears the greatest cost for misclassification? Discuss with the class on Ed Discussions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
